{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data for this project comes directly from the New York State Independent Service Authority (NYISO), which acts as a broker for the state's energy market. They store all their historical load data (on a region-wide basis) as csvs on an FTP server. \n",
    "\n",
    "You can read more here: http://www.nyiso.com/public/markets_operations/market_data/load_data/index.jsp\n",
    "\n",
    "Data was downloaded directly from here: `http://mis.nyiso.com/public/P-58Blist.htm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Electricity in New York State is generated in 12 'zones', which ahve their own separate markets. See the image below for reference. In this data, the load is split out by zones, as in the 'name' column\n",
    "\n",
    "<img src='https://business.directenergy.com/~/media/deb/images/callouts/map.ashx?la=en&h=500'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dyawitz/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import urllib\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Downloading the data\n",
    "Let's start by pulling all the data for the state for 2001-2015.\n",
    "\n",
    "The files will come in as zip files, which will go in the `../data/nyiso` folder. \n",
    "\n",
    "Then, I'll extract every csv into the `../data/nyiso/all/raw` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dates = pd.date_range(pd.to_datetime('2001-01-01'), \\\n",
    "                       pd.to_datetime('2015-12-31'), freq='M')\n",
    "\n",
    "for date in dates:\n",
    "    url = 'http://mis.nyiso.com/public/csv/pal/{0}{1}01pal_csv.zip'.format(date.year, str(date.month).zfill(2))\n",
    "    urllib.urlretrieve(url, \"../data/nyiso/{0}\".format(url.split('/')[-1]))\n",
    "\n",
    "def unzip(source_filename, dest_dir):\n",
    "    with zipfile.ZipFile(source_filename) as zf:\n",
    "        zf.extractall(dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zips = []\n",
    "for file in os.listdir(\"../data/nyiso\"):\n",
    "    if file.endswith(\".zip\"):\n",
    "        zips.append(file)\n",
    "for z in zips:\n",
    "    unzip('../data/nyiso/' + z, '../data/nyiso/all/raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all csvs into one file: combined.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csvs = []\n",
    "for file in os.listdir(\"../data/nyiso/all/raw\"):\n",
    "    if file.endswith(\"pal.csv\"):\n",
    "        csvs.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fout=open(\"../data/nyiso/all/combined.csv\",\"a\")\n",
    "\n",
    "# write the entire first file:\n",
    "for line in open(\"../data/nyiso/all/raw/\"+csvs[0]):\n",
    "    fout.write(line)\n",
    "# now the rest, skipping the headers:    \n",
    "for file in csvs[1:]:\n",
    "    f = open(\"../data/nyiso/all/raw/\"+file)\n",
    "    f.next() # skip the header\n",
    "    for line in f:\n",
    "         fout.write(line)\n",
    "    f.close() # not really needed\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and viewing the data\n",
    "\n",
    "Let's load 14 years of data into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/nyiso/all/combined.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, clean out any headers that may be in the data and just take the four columns we're interested in: timestamp, region name, id, and load (where load is electricity demand, in Megawatts). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = df.columns\n",
    "df.columns = [col.lower().replace(' ', '') for col in cols]\n",
    "df = df[['timestamp', 'name', 'ptid', 'load']][df.load != 'Load']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Rewrite this data to the csv for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('../data/nyiso/all/combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CAPITL', 'CENTRL', 'DUNWOD', 'GENESE', 'HUD VL', 'MHK VL',\n",
       "       'MILLWD', 'N.Y.C._LONGIL', 'NORTH', 'WEST', 'LONGIL', 'N.Y.C.'], dtype=object)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.name.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dictionary of weather stations\n",
    "\n",
    "We'll need this for later. Now that I have the names of each region as they're represented by the ISO, I took each and looked up the corresponding city and weather station. I put these into a dictionary and pickled it to call in the `03_get_weather_data` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regions = list(df.name.unique())\n",
    "region_names = ['Capital', 'Central', 'Dunwoodie', 'Genese', 'Hudson Valley', 'Long Island', 'Mohawk Valley', 'Millwood', 'NYC', 'North', 'West']\n",
    "cities = ['Albany', 'Syracuse', 'Yonkers', 'Rochester', 'Poughkeepsie', 'NYC', 'Utica', 'Yonkers', 'NYC', 'Plattsburgh', 'Buffalo']\n",
    "weather_stations = ['kalb', 'ksyr', 'klga', 'kroc', 'kpou', 'kjfk', 'krme', 'klga', 'kjfk', 'kpbg', 'kbuf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CAPITL': ('kalb', 'Capital', 'Albany'),\n",
       " 'CENTRL': ('ksyr', 'Central', 'Syracuse'),\n",
       " 'DUNWOD': ('klga', 'Dunwoodie', 'Yonkers'),\n",
       " 'GENESE': ('kroc', 'Genese', 'Rochester'),\n",
       " 'HUD VL': ('kpou', 'Hudson Valley', 'Poughkeepsie'),\n",
       " 'LONGIL': ('kjfk', 'Long Island', 'NYC'),\n",
       " 'MHK VL': ('krme', 'Mohawk Valley', 'Utica'),\n",
       " 'MILLWD': ('klga', 'Millwood', 'Yonkers'),\n",
       " 'N.Y.C.': ('kjfk', 'NYC', 'NYC'),\n",
       " 'NORTH': ('kpbg', 'North', 'Plattsburgh'),\n",
       " 'WEST': ('kbuf', 'West', 'Buffalo')}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_dict = dict(zip(regions, zip(weather_stations, region_names, cities)))\n",
    "weather_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weather_dict.pkl']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(weather_dict, 'weather_dict.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Take an subset of the data for each zone. A smaller spaital resoluiton will make tying in weather data more accurate.\n",
    "\n",
    "Plus, it makes the file easier to work with one at a time for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for region in weather_dict.keys():\n",
    "    subset = df[df.name == region].copy()\n",
    "    filename = weather_dict[region][1].lower().replace(' ', '') + '.csv'\n",
    "    subset.to_csv('../data/nyiso/all/' + filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>name</th>\n",
       "      <th>ptid</th>\n",
       "      <th>load</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-01-01 00:00:00</td>\n",
       "      <td>CAPITL</td>\n",
       "      <td>61757</td>\n",
       "      <td>1084.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-01 00:05:00</td>\n",
       "      <td>CAPITL</td>\n",
       "      <td>61757</td>\n",
       "      <td>1055.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-01 00:10:00</td>\n",
       "      <td>CAPITL</td>\n",
       "      <td>61757</td>\n",
       "      <td>1056.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-01-01 00:15:00</td>\n",
       "      <td>CAPITL</td>\n",
       "      <td>61757</td>\n",
       "      <td>1050.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-01-01 00:20:00</td>\n",
       "      <td>CAPITL</td>\n",
       "      <td>61757</td>\n",
       "      <td>1050.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp    name   ptid    load\n",
       "0 2012-01-01 00:00:00  CAPITL  61757  1084.4\n",
       "1 2012-01-01 00:05:00  CAPITL  61757  1055.3\n",
       "2 2012-01-01 00:10:00  CAPITL  61757  1056.6\n",
       "3 2012-01-01 00:15:00  CAPITL  61757  1050.8\n",
       "4 2012-01-01 00:20:00  CAPITL  61757  1050.8"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here's what one of those would look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output 2012 data to test on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "capital[capital.timestamp < pd.to_datetime('2013-01-01')].to_csv('load2012.csv', index=False)\n",
    "csvs = []\n",
    "for file in os.listdir(\"../data/wunderground/kalb\"):\n",
    "    if file.startswith(\"2012\"):\n",
    "        csvs.append(file)\n",
    "\n",
    "fout=open(\"weather2012.csv\",\"a\")\n",
    "\n",
    "# write the entire first file:\n",
    "for line in open(\"../data/wunderground/kalb/\"+csvs[0]):\n",
    "    fout.write(line)\n",
    "# now the rest, skipping the headers:    \n",
    "for file in csvs[1:]:\n",
    "    f = open(\"../data/wunderground/kalb/\"+file)\n",
    "    f.next() # skip the header\n",
    "    for line in f:\n",
    "         fout.write(line)\n",
    "    f.close() # not really needed\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Download historical forecasts\n",
    "The NYISO publishes a \"day-ahead\" forecast. One of the goals of this project is to see if I can outperform that. So I'll download their archived day-ahead forecast for 2014-206\n",
    "\n",
    "Use the day-ahead forecasts from the NYISO website: http://www.nyiso.com/public/markets_operations/market_data/custom_report/index.jsp?report=load_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nyiso_forecast = pd.read_csv('../data/nyiso_dayahead_forecasts/forecast_2014_2016.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211442"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nyiso_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nyiso_forecast.columns = ['timestamp', 'zone', 'forecast', 'gmt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrap notes (feel free to ignore)\n",
    "\n",
    "These are a few manual things I tried to compare my model to the forecast for Albany. Consider these cells still under development :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "capital_forecast = nyiso_forecast[nyiso_forecast.zone == 'CAPITL']\n",
    "capital_forecast.reset_index(inplace=True)\n",
    "capital_forecast = capital_forecast[['timestamp', 'zone', 'forecast']]\n",
    "capital_forecast.to_csv('../data/nyiso_dayahead_forecasts/capital_forecast.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data was copied in raw from weather.gov. It's the Albany 48 hour temperature foreacst that went into the final pdf presentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forecast_48 = [46,47,47,46,45,43,42,41,41,40,39,38,37,36,36,37,38,40,41,43,44,44,45,45,45,43,41,38,36,34,32,31,30,28,27,26,26,25,25,28,34,39,43,47,51,54,55,56,56,52,49,47,46,45,45,44,42,41,40,39,39,39,39,42,46,50,54,57,60,62,63,64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phour = pd.to_datetime('1 hour')\n",
    "start_time = pd.to_datetime('2016-03-28')\n",
    "[range(1,73)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
